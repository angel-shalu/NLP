{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "39f97dac-e81c-440d-9fa5-fea6d6f62c0c",
   "metadata": {},
   "source": [
    "# Extractive Text Summarization with spaCy Project :\n",
    "\n",
    "- **Goal:** Build an extractive summarizer (selects the most important sentences from the original text).\n",
    "- **Why:** Compress long documents into concise overviews for faster reading and downstream analysis.\n",
    "- **Method:** Rank sentences by content importance using token statistics (term frequency), then pick the top-K."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "835172ed-be1e-42a7-8d71-b14f9e6bd974",
   "metadata": {},
   "source": [
    "# Environment & Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "db87255d-e06f-4f3f-b764-d4ed77327071",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install spacy\n",
    "\n",
    "# 3 English pipelines exist by size/quality/speed: sm (small), md (medium), lg (large)\n",
    "#!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58f98974-2888-48de-87f8-3859041d8328",
   "metadata": {},
   "source": [
    "# Imports & Pipeline Configuration\n",
    "\n",
    "- spaCy provides a processing pipeline that turns raw text → Doc with tokens, sentences, and linguistic annotations.\n",
    "\n",
    "- We add a sentencizer (rule-based sentence boundary detector) so sentence segmentation works even if we disable heavier components (faster).\n",
    "\n",
    "- We’ll use stopword and punctuation filtering to remove low-information tokens from scoring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "32d90815-4082-4c08-802d-ef5728c8f2b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "from string import punctuation\n",
    "from heapq import nlargest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "692235dd-0037-45b0-9242-0945840cd584",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load lightweight English model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Ensure we have a cheap, reliable sentence splitter\n",
    "# (If the parser is active, it's already good; sentencizer is fast & deterministic.)\n",
    "if \"sentencizer\" not in nlp.pipe_names:\n",
    "    nlp.add_pipe(\"sentencizer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfa0546a-87b9-45ed-b9ae-1ad102ae7196",
   "metadata": {},
   "source": [
    "# Input Text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb710d98-cdfd-4506-932f-abed0d50841e",
   "metadata": {},
   "source": [
    "- Any raw string can be processed. Later you’ll replace this with file ingestion (TXT/PDF/DOCX).\n",
    "\n",
    "- We keep original casing/punctuation for the final summary readability, but we lowercase for scoring to avoid case bias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ec6a2e31-0e06-46e7-aec2-f1693c5ea38c",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"\n",
    "There are broadly two types of extractive summarization tasks depending on what the summarization program focuses on.\n",
    "The first is generic summarization, which focuses on obtaining a generic summary or abstract of the collection (whether documents, or sets of images, or videos, news stories etc.).\n",
    "The second is query relevant summarization, sometimes called query-based summarization, which summarizes objects specific to a query.\n",
    "Summarization systems are able to create both query relevant text summaries and generic machine-generated summaries depending on what the user needs.\n",
    "An example of a summarization problem is document summarization, which attempts to automatically produce an abstract from a given document.\n",
    "Sometimes one might be interested in generating a summary from a single source document, while others can use multiple source documents (for example, a cluster of articles on the same topic).\n",
    "This problem is called multi-document summarization.\n",
    "A related application is summarizing news articles.\n",
    "Imagine a system which automatically pulls together news articles on a given topic (from the web), and concisely represents the latest news as a summary.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a46498ee-81cf-47bd-89f9-51a6c2c8f060",
   "metadata": {},
   "source": [
    "# Tokenization & Linguistic Annotations\n",
    "\n",
    "- nlp(text) → Doc: a container of Token objects with rich attributes (e.g., text, lemma_, is_stop).\n",
    "\n",
    "- We’ll score words using term frequency (TF). Optionally, we can score by lemma (group run/running/ran)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9576fa91-cb41-4f5a-a84f-3c55b2250187",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\n',\n",
       " 'There',\n",
       " 'are',\n",
       " 'broadly',\n",
       " 'two',\n",
       " 'types',\n",
       " 'of',\n",
       " 'extractive',\n",
       " 'summarization',\n",
       " 'tasks',\n",
       " 'depending',\n",
       " 'on',\n",
       " 'what',\n",
       " 'the',\n",
       " 'summarization',\n",
       " 'program',\n",
       " 'focuses',\n",
       " 'on',\n",
       " '.',\n",
       " '\\n']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = nlp(text)\n",
    "\n",
    "# Peek at tokens (debug)\n",
    "tokens_preview = [t.text for t in doc[:20]]\n",
    "tokens_preview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19a139df-72f4-45d5-8009-e555f474f561",
   "metadata": {},
   "source": [
    "# Vocabulary Pruning & Weighting\n",
    "\n",
    "- Build a vocabulary of informative terms.\n",
    "\n",
    "- Remove stopwords (common function words) and punctuation (non-lexical).\n",
    "\n",
    "- Optionally include digits/symbols depending on domain (e.g., finance).\n",
    "\n",
    "**Why:**\n",
    "\n",
    "- Reduces noise. Keeps only content-bearing terms that better correlate with sentence salience.\n",
    "\n",
    "**Design choices:**\n",
    "\n",
    "- Use lemma to merge inflectional variants (recommended).\n",
    "\n",
    "- Normalize TF by L∞ norm (divide by max frequency) to keep scores in 0,1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "90d686dd-af18-4950-85d5-3b04f2f49630",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'broadly': 0.1111111111111111,\n",
       " 'type': 0.1111111111111111,\n",
       " 'extractive': 0.1111111111111111,\n",
       " 'summarization': 1.0,\n",
       " 'task': 0.1111111111111111,\n",
       " 'depend': 0.2222222222222222,\n",
       " 'program': 0.1111111111111111,\n",
       " 'focus': 0.2222222222222222,\n",
       " 'generic': 0.3333333333333333,\n",
       " 'obtain': 0.1111111111111111,\n",
       " 'summary': 0.5555555555555556,\n",
       " 'abstract': 0.2222222222222222,\n",
       " 'collection': 0.1111111111111111,\n",
       " 'document': 0.6666666666666666,\n",
       " 'set': 0.1111111111111111,\n",
       " 'image': 0.1111111111111111,\n",
       " 'video': 0.1111111111111111,\n",
       " 'news': 0.4444444444444444,\n",
       " 'story': 0.1111111111111111,\n",
       " 'etc': 0.1111111111111111,\n",
       " 'second': 0.1111111111111111,\n",
       " 'query': 0.4444444444444444,\n",
       " 'relevant': 0.2222222222222222,\n",
       " 'base': 0.1111111111111111,\n",
       " 'summarize': 0.2222222222222222,\n",
       " 'object': 0.1111111111111111,\n",
       " 'specific': 0.1111111111111111,\n",
       " 'system': 0.2222222222222222,\n",
       " 'able': 0.1111111111111111,\n",
       " 'create': 0.1111111111111111,\n",
       " 'text': 0.1111111111111111,\n",
       " 'machine': 0.1111111111111111,\n",
       " 'generate': 0.2222222222222222,\n",
       " 'user': 0.1111111111111111,\n",
       " 'need': 0.1111111111111111,\n",
       " 'example': 0.2222222222222222,\n",
       " 'problem': 0.2222222222222222,\n",
       " 'attempt': 0.1111111111111111,\n",
       " 'automatically': 0.2222222222222222,\n",
       " 'produce': 0.1111111111111111,\n",
       " 'interested': 0.1111111111111111,\n",
       " 'single': 0.1111111111111111,\n",
       " 'source': 0.2222222222222222,\n",
       " 'use': 0.1111111111111111,\n",
       " 'multiple': 0.1111111111111111,\n",
       " 'cluster': 0.1111111111111111,\n",
       " 'article': 0.3333333333333333,\n",
       " 'topic': 0.2222222222222222,\n",
       " 'multi': 0.1111111111111111,\n",
       " 'related': 0.1111111111111111,\n",
       " 'application': 0.1111111111111111,\n",
       " 'imagine': 0.1111111111111111,\n",
       " 'pull': 0.1111111111111111,\n",
       " 'web': 0.1111111111111111,\n",
       " 'concisely': 0.1111111111111111,\n",
       " 'represent': 0.1111111111111111,\n",
       " 'late': 0.1111111111111111}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords = STOP_WORDS\n",
    "punct_set = set(punctuation)\n",
    "\n",
    "use_lemma = True   # switch to False to use surface forms\n",
    "\n",
    "word_freq = {}\n",
    "for token in doc:\n",
    "    if token.is_space or token.is_punct:\n",
    "        continue\n",
    "    if token.is_stop:\n",
    "        continue\n",
    "    if token.text in punct_set:\n",
    "        continue\n",
    "    \n",
    "    key = token.lemma_.lower() if use_lemma else token.text.lower()\n",
    "    if not key or key in stopwords:\n",
    "        continue\n",
    "    word_freq[key] = word_freq.get(key, 0) + 1\n",
    "\n",
    "# Normalize by max frequency (L∞ normalization)\n",
    "if word_freq:\n",
    "    max_f = max(word_freq.values())\n",
    "    for w in word_freq:\n",
    "        word_freq[w] = word_freq[w] / max_f\n",
    "\n",
    "word_freq"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64a814f2-b113-47ef-adf0-75f63b49b053",
   "metadata": {},
   "source": [
    "> Note: L∞ normalization is simple and stable. Alternatives: L1 (sum to 1), TF-IDF (requires document collection; better when many docs)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30ab4159-8a4d-469f-bc9f-4d73bbab9087",
   "metadata": {},
   "source": [
    "# Sentence Segmentation\n",
    "\n",
    "- Build a list of candidate sentences to rank.\n",
    "- Sentences come from Doc.sents (via sentencizer or parser).\n",
    "\n",
    "- We’ll preserve original order later for readability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "895bac6b-da18-4127-aac5-d235bc2c3814",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9,\n",
       " [\n",
       "  There are broadly two types of extractive summarization tasks depending on what the summarization program focuses on.,\n",
       "  The first is generic summarization, which focuses on obtaining a generic summary or abstract of the collection (whether documents, or sets of images, or videos, news stories etc.).,\n",
       "  The second is query relevant summarization, sometimes called query-based summarization, which summarizes objects specific to a query.])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences = list(doc.sents)\n",
    "len(sentences), sentences[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f57ca43b-dde0-47ef-9bc1-d47ea16c6a5e",
   "metadata": {},
   "source": [
    "# Sentence Scoring\n",
    "\n",
    "- Score each sentence by summing the normalized token weights of its content words.\n",
    "\n",
    "- To reduce length bias (long sentences get larger sums), we can length-normalize by sentence token count.\n",
    "\n",
    "**Why:**\n",
    "\n",
    "- Simple additive content model approximates importance: sentences containing many high-value terms score higher."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e8d38ef8-09da-43a1-bc0b-c0f7214abdbf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\n",
       " There are broadly two types of extractive summarization tasks depending on what the summarization program focuses on.: 0.17647058823529413,\n",
       " The first is generic summarization, which focuses on obtaining a generic summary or abstract of the collection (whether documents, or sets of images, or videos, news stories etc.).: 0.16269841269841265,\n",
       " The second is query relevant summarization, sometimes called query-based summarization, which summarizes objects specific to a query.: 0.23456790123456792,\n",
       " Summarization systems are able to create both query relevant text summaries and generic machine-generated summaries depending on what the user needs.: 0.202020202020202,\n",
       " An example of a summarization problem is document summarization, which attempts to automatically produce an abstract from a given document.: 0.22222222222222224,\n",
       " Sometimes one might be interested in generating a summary from a single source document, while others can use multiple source documents (for example, a cluster of articles on the same topic).: 0.12544802867383514,\n",
       " This problem is called multi-document summarization.: 0.2857142857142857,\n",
       " A related application is summarizing news articles.: 0.1746031746031746,\n",
       " Imagine a system which automatically pulls together news articles on a given topic (from the web), and concisely represents the latest news as a summary.: 0.12444444444444443}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "length_normalize = True\n",
    "\n",
    "sent_scores = {}\n",
    "for sent in sentences:\n",
    "    score = 0.0\n",
    "    length = 0\n",
    "    for token in sent:\n",
    "        if token.is_space or token.is_punct:\n",
    "            continue\n",
    "        key = (token.lemma_.lower() if use_lemma else token.text.lower())\n",
    "        if key in word_freq:\n",
    "            score += word_freq[key]\n",
    "        length += 1\n",
    "    \n",
    "    if length == 0:\n",
    "        continue\n",
    "    \n",
    "    if length_normalize:\n",
    "        score = score / length  # mean weight per token\n",
    "    \n",
    "    sent_scores[sent] = score\n",
    "\n",
    "sent_scores\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "253fbfe3-5c1b-43e8-82ed-19be08cf2d1c",
   "metadata": {},
   "source": [
    "> Alternatives:\n",
    "\n",
    "- Positional prior: Boost early sentences (useful for news).\n",
    "\n",
    "- Title overlap: Boost terms appearing in the document title.\n",
    "\n",
    "- Redundancy control / Diversity: Penalize sentences that repeat selected content (see MMR below)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1a07715-256e-476d-b0a2-f8dbd7335e35",
   "metadata": {},
   "source": [
    "# Selection (Top-K or Ratio)\n",
    "\n",
    "- Choose K sentences (or ratio of total) with highest scores using heapq.nlargest.\n",
    "\n",
    "- Then restore original document order for readability.\n",
    "\n",
    "**Why:**\n",
    "\n",
    "- Ranking → selection is the heart of extractive summarization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c8d5f610-e9ef-4209-a7b6-52de61496e2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The second is query relevant summarization, sometimes called query-based summarization, which summarizes objects specific to a query. An example of a summarization problem is document summarization, which attempts to automatically produce an abstract from a given document. This problem is called multi-document summarization.'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "ratio = 0.3                 # keep 30% sentences (set 0.2–0.35 for readable summaries)\n",
    "min_sentences = 3\n",
    "max_sentences = 8\n",
    "\n",
    "K = max(min_sentences, int(len(sentences) * ratio))\n",
    "K = min(K, max_sentences, len(sentences))\n",
    "\n",
    "top_sents = nlargest(K, sent_scores, key=sent_scores.get)\n",
    "\n",
    "# Restore original order\n",
    "top_sents_sorted = sorted(top_sents, key=lambda s: s.start)\n",
    "\n",
    "summary_text = \" \".join([s.text.strip() for s in top_sents_sorted])\n",
    "summary_text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90daca35-d76c-4304-912a-beba198ff249",
   "metadata": {},
   "source": [
    "# (Optional) Redundancy Reduction with MMR (Maximal Marginal Relevance)\n",
    "\n",
    "- Pure top-K may select near-duplicate sentences.\n",
    "\n",
    "- MMR trades off relevance (sentence score) with novelty (dissimilarity to already chosen sentences).\n",
    "\n",
    "- We approximate similarity with token overlap (Jaccard) for simplicity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6a199ca5-c8fb-481b-9966-d849455f74c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The second is query relevant summarization, sometimes called query-based summarization, which summarizes objects specific to a query. An example of a summarization problem is document summarization, which attempts to automatically produce an abstract from a given document. This problem is called multi-document summarization.'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def jaccard(a_tokens, b_tokens):\n",
    "    a, b = set(a_tokens), set(b_tokens)\n",
    "    if not a or not b: \n",
    "        return 0.0\n",
    "    return len(a & b) / len(a | b)\n",
    "\n",
    "def select_with_mmr(sentences, scores, K, lambda_=0.7):\n",
    "    # lambda_: 1.0 favors relevance only; 0.0 favors novelty only\n",
    "    selected = []\n",
    "    remaining = set(sentences)\n",
    "    while remaining and len(selected) < K:\n",
    "        if not selected:\n",
    "            # pick the most relevant first\n",
    "            best = max(remaining, key=lambda s: scores.get(s, 0))\n",
    "            selected.append(best)\n",
    "            remaining.remove(best)\n",
    "            continue\n",
    "        \n",
    "        def mmr_score(s):\n",
    "            sim_to_sel = max(\n",
    "                jaccard([t.lemma_.lower() for t in s if t.is_alpha],\n",
    "                        [t.lemma_.lower() for t in x if t.is_alpha]) \n",
    "                for x in selected\n",
    "            ) if selected else 0.0\n",
    "            return lambda_ * scores.get(s, 0) - (1 - lambda_) * sim_to_sel\n",
    "        \n",
    "        best = max(remaining, key=mmr_score)\n",
    "        selected.append(best)\n",
    "        remaining.remove(best)\n",
    "    return sorted(selected, key=lambda s: s.start)\n",
    "\n",
    "mmr_sents = select_with_mmr(sentences, sent_scores, K, lambda_=0.75)\n",
    "mmr_summary = \" \".join([s.text.strip() for s in mmr_sents])\n",
    "mmr_summary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06aa89e3-1282-43b9-b89e-91505bd5fa41",
   "metadata": {},
   "source": [
    "# Packaging as a Reusable Function\n",
    "\n",
    "* Encapsulate the pipeline for reuse (backend API / UI).\n",
    "\n",
    "* Parameters expose trade-offs: ratio, lemma use, normalization, MMR, limits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4c5cc8a4-b7cc-4052-ad69-dfa57611a42e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The second is query relevant summarization, sometimes called query-based summarization, which summarizes objects specific to a query. An example of a summarization problem is document summarization, which attempts to automatically produce an abstract from a given document. This problem is called multi-document summarization.'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def summarize_text_spacy(\n",
    "    text: str,\n",
    "    nlp,\n",
    "    ratio: float = 0.3,\n",
    "    min_sentences: int = 3,\n",
    "    max_sentences: int = 8,\n",
    "    use_lemma: bool = True,\n",
    "    length_normalize: bool = True,\n",
    "    use_mmr: bool = False,\n",
    "    mmr_lambda: float = 0.75\n",
    ") -> str:\n",
    "    if not text or not text.strip():\n",
    "        return \"\"\n",
    "    \n",
    "    doc = nlp(text)\n",
    "    sentences = list(doc.sents)\n",
    "    if not sentences:\n",
    "        return text.strip()\n",
    "    \n",
    "    stopwords = STOP_WORDS\n",
    "    punct_set = set(punctuation)\n",
    "    \n",
    "    word_freq = {}\n",
    "    for token in doc:\n",
    "        if token.is_space or token.is_punct:\n",
    "            continue\n",
    "        if token.is_stop:\n",
    "            continue\n",
    "        if token.text in punct_set:\n",
    "            continue\n",
    "        key = token.lemma_.lower() if use_lemma else token.text.lower()\n",
    "        if not key or key in stopwords:\n",
    "            continue\n",
    "        word_freq[key] = word_freq.get(key, 0) + 1\n",
    "    \n",
    "    if not word_freq:\n",
    "        # fallback: return lead sentences\n",
    "        keep = max(min_sentences, int(len(sentences) * ratio))\n",
    "        keep = min(keep, max_sentences, len(sentences))\n",
    "        return \" \".join([s.text.strip() for s in sentences[:keep]])\n",
    "    \n",
    "    max_f = max(word_freq.values())\n",
    "    for w in word_freq:\n",
    "        word_freq[w] = word_freq[w] / max_f  # L∞ normalization\n",
    "    \n",
    "    sent_scores = {}\n",
    "    for sent in sentences:\n",
    "        score = 0.0\n",
    "        length = 0\n",
    "        for token in sent:\n",
    "            if token.is_space or token.is_punct:\n",
    "                continue\n",
    "            key = token.lemma_.lower() if use_lemma else token.text.lower()\n",
    "            if key in word_freq:\n",
    "                score += word_freq[key]\n",
    "            length += 1\n",
    "        if length == 0:\n",
    "            continue\n",
    "        if length_normalize:\n",
    "            score /= length\n",
    "        sent_scores[sent] = score\n",
    "    \n",
    "    K = max(min_sentences, int(len(sentences) * ratio))\n",
    "    K = min(K, max_sentences, len(sentences))\n",
    "    \n",
    "    if use_mmr:\n",
    "        chosen = select_with_mmr(sentences, sent_scores, K, lambda_=mmr_lambda)\n",
    "    else:\n",
    "        top = nlargest(K, sent_scores, key=sent_scores.get)\n",
    "        chosen = sorted(top, key=lambda s: s.start)\n",
    "    \n",
    "    return \" \".join([s.text.strip() for s in chosen])\n",
    "\n",
    "# Demo\n",
    "summary_text = summarize_text_spacy(text, nlp, ratio=0.3, use_mmr=True)\n",
    "summary_text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4aa9428-7d26-46b2-adf1-4d16fccf88fd",
   "metadata": {},
   "source": [
    "# Complexity & Behavior\n",
    "\n",
    "* Time:\n",
    "    * Token pass (vocab build): O(N) tokens\n",
    "\n",
    "    * Sentence scoring: O(N) tokens\n",
    "\n",
    "    * Selection: O(S log K) with heap (S=sentences)\n",
    "\n",
    "* Space: vocabulary O(V), sentence scores O(S)\n",
    "\n",
    "* Determinism: deterministic given same text & parameters (no randomness).\n",
    "\n",
    "* Biases: favors content-dense sentences; length normalization mitigates long-sentence bias; MMR mitigates redundancy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df11d12c-78a0-449d-a266-b3d39c928b67",
   "metadata": {},
   "source": [
    "# Limitations & Upgrades\n",
    "\n",
    "* Extractive only: doesn’t paraphrase or compress internally (no “abstractive” fluency improvements).\n",
    "\n",
    "* Vocabulary mismatch: rare words can be overweighted; consider TF-IDF over a corpus.\n",
    "\n",
    "* Domain sensitivity: customize stopwords, keep numbers/symbols if domain requires (finance, science).\n",
    "\n",
    "* Improvements:\n",
    "\n",
    "    * TextRank (graph centrality)\n",
    "\n",
    "    * Supervised extractive models\n",
    "\n",
    "    * Abstractive LLMs for fluent summaries (cost/latency/safety trade-offs)\n",
    "\n",
    "    * ROUGE for evaluation against reference summaries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8b6db3c-963d-4ca0-9c9e-2018beba7c35",
   "metadata": {},
   "source": [
    "# Frontend Code (Streamlit App)\n",
    "\n",
    "Save this as app.py:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50f93a8f-0f9a-4566-80bd-00512d9b3679",
   "metadata": {},
   "source": [
    "```python\n",
    "import streamlit as st\n",
    "import spacy\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "from string import punctuation\n",
    "from heapq import nlargest\n",
    "\n",
    "# =============================\n",
    "# Summarizer Function\n",
    "# =============================\n",
    "def summarize_text_spacy(\n",
    "    text: str,\n",
    "    nlp,\n",
    "    ratio: float = 0.3,\n",
    "    min_sentences: int = 3,\n",
    "    max_sentences: int = 8\n",
    ") -> str:\n",
    "    if not text or not text.strip():\n",
    "        return \"\"\n",
    "    \n",
    "    doc = nlp(text)\n",
    "    sentences = list(doc.sents)\n",
    "    if not sentences:\n",
    "        return text.strip()\n",
    "    \n",
    "    stopwords = STOP_WORDS\n",
    "    punct_set = set(punctuation)\n",
    "    \n",
    "    word_freq = {}\n",
    "    for token in doc:\n",
    "        if token.is_space or token.is_punct:\n",
    "            continue\n",
    "        if token.is_stop:\n",
    "            continue\n",
    "        if token.text in punct_set:\n",
    "            continue\n",
    "        key = token.lemma_.lower()\n",
    "        if not key or key in stopwords:\n",
    "            continue\n",
    "        word_freq[key] = word_freq.get(key, 0) + 1\n",
    "    \n",
    "    if not word_freq:\n",
    "        return \" \".join([s.text.strip() for s in sentences[:min_sentences]])\n",
    "    \n",
    "    max_f = max(word_freq.values())\n",
    "    for w in word_freq:\n",
    "        word_freq[w] = word_freq[w] / max_f\n",
    "    \n",
    "    sent_scores = {}\n",
    "    for sent in sentences:\n",
    "        score = 0.0\n",
    "        length = 0\n",
    "        for token in sent:\n",
    "            if token.is_space or token.is_punct:\n",
    "                continue\n",
    "            key = token.lemma_.lower()\n",
    "            if key in word_freq:\n",
    "                score += word_freq[key]\n",
    "            length += 1\n",
    "        if length > 0:\n",
    "            score /= length\n",
    "            sent_scores[sent] = score\n",
    "    \n",
    "    K = max(min_sentences, int(len(sentences) * ratio))\n",
    "    K = min(K, max_sentences, len(sentences))\n",
    "    \n",
    "    top = nlargest(K, sent_scores, key=sent_scores.get)\n",
    "    chosen = sorted(top, key=lambda s: s.start)\n",
    "    \n",
    "    return \" \".join([s.text.strip() for s in chosen])\n",
    "\n",
    "# =============================\n",
    "# Streamlit Frontend\n",
    "# =============================\n",
    "st.set_page_config(page_title=\"Text Summarizer\", page_icon=\"📝\", layout=\"wide\")\n",
    "st.title(\"📝 Extractive Text Summarizer\")\n",
    "st.write(\"Upload a file or paste text below to generate a summary.\")\n",
    "\n",
    "# Load spaCy model once\n",
    "@st.cache_resource\n",
    "def load_model():\n",
    "    return spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "nlp = load_model()\n",
    "\n",
    "# Sidebar controls\n",
    "st.sidebar.header(\"⚙️ Settings\")\n",
    "ratio = st.sidebar.slider(\"Summary Length (ratio of original)\", 0.1, 0.9, 0.3, 0.05)\n",
    "min_sents = st.sidebar.number_input(\"Minimum Sentences\", 1, 10, 3)\n",
    "max_sents = st.sidebar.number_input(\"Maximum Sentences\", 1, 20, 8)\n",
    "\n",
    "# Input section\n",
    "input_method = st.radio(\"Choose Input Method:\", [\"Paste Text\", \"Upload File\"])\n",
    "\n",
    "input_text = \"\"\n",
    "if input_method == \"Paste Text\":\n",
    "    input_text = st.text_area(\"Enter your text here:\", height=200)\n",
    "elif input_method == \"Upload File\":\n",
    "    uploaded_file = st.file_uploader(\"Upload a .txt, .pdf, or .docx file\", type=[\"txt\", \"pdf\", \"docx\"])\n",
    "    if uploaded_file:\n",
    "        ext = uploaded_file.name.split(\".\")[-1].lower()\n",
    "        if ext == \"txt\":\n",
    "            input_text = uploaded_file.read().decode(\"utf-8\")\n",
    "        elif ext == \"pdf\":\n",
    "            import pdfplumber\n",
    "            with pdfplumber.open(uploaded_file) as pdf:\n",
    "                input_text = \"\\n\".join(page.extract_text() or \"\" for page in pdf.pages)\n",
    "        elif ext == \"docx\":\n",
    "            from docx import Document\n",
    "            doc = Document(uploaded_file)\n",
    "            input_text = \"\\n\".join(p.text for p in doc.paragraphs)\n",
    "\n",
    "# Run summarization\n",
    "if st.button(\"Generate Summary\"):\n",
    "    if input_text.strip():\n",
    "        summary = summarize_text_spacy(input_text, nlp, ratio=ratio, min_sentences=min_sents, max_sentences=max_sents)\n",
    "        st.subheader(\"📄 Original Text\")\n",
    "        st.write(input_text)\n",
    "        st.subheader(\"✂️ Generated Summary\")\n",
    "        st.success(summary)\n",
    "    else:\n",
    "        st.warning(\"Please provide text or upload a file first.\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d17b8421-bec0-4bdb-ba3a-0502fc258260",
   "metadata": {},
   "source": [
    "# How to Run\n",
    "\n",
    "1. Save as app.py.\n",
    "\n",
    "2. Install dependencies:\n",
    "``` python \n",
    "pip install streamlit spacy pdfplumber python-docx\n",
    "python -m spacy download en_core_web_sm\n",
    "```\n",
    "3. Run:\n",
    "``` python\n",
    "streamlit run app.py\n",
    "```\n",
    "4. Browser will open → paste text or upload file → see summary."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02517417-d880-42a9-a4d4-d9f0c48ac341",
   "metadata": {},
   "source": [
    "## Overall App Look\n",
    "\n",
    "This shows the complete UI of our summarizer app with background image and controls."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3019daa5-d281-46ae-822e-925efc94ec35",
   "metadata": {},
   "source": [
    "![Overall App Look](Out_1.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3428dea9-3b34-43ee-a6c3-f33a31244e35",
   "metadata": {},
   "source": [
    "## Upload Text File & Generate Summary\n",
    "\n",
    "Here we uploaded a .txt file, extracted its content, and generated a summary."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d0f6e23-a5d8-4eca-8f71-c39e50b4a0b0",
   "metadata": {},
   "source": [
    "![Uploaded File Result](Out_2.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7feb4809-eb64-476c-a2a8-4d8d971ec216",
   "metadata": {},
   "source": [
    "## Summary Length = 50%\n",
    "\n",
    "When we set the summary ratio to 50%, the app extracted half of the original text into a concise summary."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8d851b6-60b9-4dc6-80ee-a044bbd528f0",
   "metadata": {},
   "source": [
    "![Summary 50%](Out_3.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c34685da-14e0-4817-a1d7-1b395297a468",
   "metadata": {},
   "source": [
    "## Minimum Sentences = 2 Lines\n",
    "\n",
    "When the minimum sentences parameter was set to 2, the app ensured at least two lines were returned as summary."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c868764-51c3-460c-9e3f-378435596bcb",
   "metadata": {},
   "source": [
    "![Sentence Length 2 Lines](Out_4.PNG)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
