{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f5353885-a505-40b4-b87f-94ab79ec8294",
   "metadata": {},
   "source": [
    "**Advanced Tokenization, N-grams, Stemming, Lemmatization & Stopwords**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47a637ae-8d86-4366-bbed-6fd76d2118e6",
   "metadata": {},
   "source": [
    "Today, we continued our journey into Natural Language Processing (NLP) by exploring more tokenization techniques, generating n-grams, and learning about stemming, lemmatization, and stopwords."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f40bac10-ba68-416e-bd69-87aef1d0e7fe",
   "metadata": {},
   "source": [
    "# Whitespace Tokenization\n",
    "\n",
    "**Definition:** Splits text based on whitespace (spaces, tabs, newlines) without removing punctuation.\n",
    "\n",
    "- Useful when punctuation should be preserved as part of the tokens.\n",
    "\n",
    "- Faster but less precise than other tokenizers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2f620cc0-698a-4db0-8b61-787a83b81f47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# you can also create your own words\n",
    "AI = '''Artificial Intelligence refers to the intelligence of machines. This is in contrast to the natural intelligence of\n",
    "humans and animals. With Artificial Intelligence, machines perform functions such as learning, planning, reasoning and\n",
    "problem-solving. Most noteworthy, Artificial Intelligence is the simulation of human intelligence by machines.\n",
    "It is probably the fastest-growing development in the World of technology and innovation. Furthermore, many experts believe\n",
    "AI could solve major challenges and crisis situations.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1244e319-36bd-412c-b10c-db23a7f1a214",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Artificial', 'Intelligence', 'refers', 'to', 'the', 'intelligence', 'of', 'machines.', 'This', 'is', 'in', 'contrast', 'to', 'the', 'natural', 'intelligence', 'of', 'humans', 'and', 'animals.', 'With', 'Artificial', 'Intelligence,', 'machines', 'perform', 'functions', 'such', 'as', 'learning,', 'planning,', 'reasoning', 'and', 'problem-solving.', 'Most', 'noteworthy,', 'Artificial', 'Intelligence', 'is', 'the', 'simulation', 'of', 'human', 'intelligence', 'by', 'machines.', 'It', 'is', 'probably', 'the', 'fastest-growing', 'development', 'in', 'the', 'World', 'of', 'technology', 'and', 'innovation.', 'Furthermore,', 'many', 'experts', 'believe', 'AI', 'could', 'solve', 'major', 'challenges', 'and', 'crisis', 'situations.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import WhitespaceTokenizer\n",
    "\n",
    "wt = WhitespaceTokenizer().tokenize(AI)\n",
    "print(wt)  # Clean split by spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0b1a055d-75d2-4a5c-96f4-73601107b6f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70\n"
     ]
    }
   ],
   "source": [
    "print(len(wt))  # Count of tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7ef1c07-1031-42d0-a5ca-6c6eb2d962c2",
   "metadata": {},
   "source": [
    "# WordPunct Tokenization\n",
    "**Definition:** Splits words and punctuation into separate tokens.\n",
    "\n",
    "- Numbers, punctuation marks, and words are treated individually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7c410985-a82d-4b01-9120-be359425c858",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Good apple cost $3.88 in Hyderabad. Please buy two of them. Thanks.'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import wordpunct_tokenize\n",
    "\n",
    "s = 'Good apple cost $3.88 in Hyderabad. Please buy two of them. Thanks.'\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "26265c39-226d-4d30-be46-6ea433b06dd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Good', 'apple', 'cost', '$', '3', '.', '88', 'in', 'Hyderabad', '.', 'Please', 'buy', 'two', 'of', 'them', '.', 'Thanks', '.']\n"
     ]
    }
   ],
   "source": [
    "print(wordpunct_tokenize(s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a42e3cc7-e01b-489f-9827-aabc3723f257",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18\n"
     ]
    }
   ],
   "source": [
    "print(len(wordpunct_tokenize(s)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cefc0774-b1c1-495a-86af-aaf0cc884c65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Artificial', 'Intelligence', 'refers', 'to', 'the', 'intelligence', 'of', 'machines', '.', 'This', 'is', 'in', 'contrast', 'to', 'the', 'natural', 'intelligence', 'of', 'humans', 'and', 'animals', '.', 'With', 'Artificial', 'Intelligence', ',', 'machines', 'perform', 'functions', 'such', 'as', 'learning', ',', 'planning', ',', 'reasoning', 'and', 'problem', '-', 'solving', '.', 'Most', 'noteworthy', ',', 'Artificial', 'Intelligence', 'is', 'the', 'simulation', 'of', 'human', 'intelligence', 'by', 'machines', '.', 'It', 'is', 'probably', 'the', 'fastest', '-', 'growing', 'development', 'in', 'the', 'World', 'of', 'technology', 'and', 'innovation', '.', 'Furthermore', ',', 'many', 'experts', 'believe', 'AI', 'could', 'solve', 'major', 'challenges', 'and', 'crisis', 'situations', '.']\n",
      "85\n"
     ]
    }
   ],
   "source": [
    "w_p = wordpunct_tokenize(AI)\n",
    "print(w_p)\n",
    "print(len(w_p))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "039c7902-e8a3-4946-ad03-d88c5bb4c443",
   "metadata": {},
   "source": [
    "**Summary of Tokenizers**\n",
    "\n",
    "| Function / Tokenizer  | Description                                                              |\n",
    "| --------------------- | ------------------------------------------------------------------------ |\n",
    "| `word_tokenize`       | Splits text into words and punctuation (accurate).                       |\n",
    "| `sent_tokenize`       | Splits text into sentences.                                              |\n",
    "| `blankline_tokenize`  | Splits text into paragraphs by blank lines.                              |\n",
    "| `WhitespaceTokenizer` | Splits by spaces/tabs only, keeps punctuation attached.                  |\n",
    "| `wordpunct_tokenize`  | Splits words and punctuation into separate tokens.                       |\n",
    "| `pos_tag`             | Assigns grammatical roles (nouns, verbs, adjectives) to each token.      |\n",
    "| `ne_chunk`            | Performs Named Entity Recognition (NER) to identify people, places, etc. |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23a765be-5237-4195-9041-eedea650145a",
   "metadata": {},
   "source": [
    "# N-grams\n",
    "**Definition:** Sequences of n consecutive tokens.\n",
    "\n",
    "- Bigram: 2-word sequence.\n",
    "- Trigram: 3-word sequence.\n",
    "- N-gram: Any n-word sequence (n > 3)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e84f76ef-b207-4bfc-a927-b70d9f64a7e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'We are learner of AI from 12th June 2025 till now'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.util import bigrams, trigrams, ngrams\n",
    "\n",
    "string = \"We are learner of AI from 12th June 2025 till now\"\n",
    "string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a0c7944c-482e-428a-b7b4-3926d7f2c2c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['We', 'are', 'learner', 'of', 'AI', 'from', '12th', 'June', '2025', 'till', 'now']\n",
      "11\n"
     ]
    }
   ],
   "source": [
    "quotes_tokens = nltk.word_tokenize(string)\n",
    "\n",
    "print(quotes_tokens)\n",
    "print(len(quotes_tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07d38133-2827-4ac0-8b7d-3f7f7ef671cb",
   "metadata": {},
   "source": [
    "## Bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "38028bf9-571d-4e9f-8298-1e7bba55c92a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('We', 'are'), ('are', 'learner'), ('learner', 'of'), ('of', 'AI'), ('AI', 'from'), ('from', '12th'), ('12th', 'June'), ('June', '2025'), ('2025', 'till'), ('till', 'now')]\n"
     ]
    }
   ],
   "source": [
    "# Bigrams\n",
    "quotes_tokens_bi = list(nltk.bigrams(quotes_tokens))\n",
    "print(quotes_tokens_bi)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcb2dc8d-54f0-4ce9-9346-13fdd490caa8",
   "metadata": {},
   "source": [
    "## Trigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c52f5ff5-88b5-4298-975a-6ab54eeca18c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('We', 'are', 'learner'), ('are', 'learner', 'of'), ('learner', 'of', 'AI'), ('of', 'AI', 'from'), ('AI', 'from', '12th'), ('from', '12th', 'June'), ('12th', 'June', '2025'), ('June', '2025', 'till'), ('2025', 'till', 'now')]\n"
     ]
    }
   ],
   "source": [
    "# Trigrams\n",
    "quotes_tokens_tri = list(nltk.trigrams(quotes_tokens))\n",
    "print(quotes_tokens_tri)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ddd8815-5749-4e9c-95c0-e65fabb175d0",
   "metadata": {},
   "source": [
    "## N-grams (n=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f8837952-bdf5-4ec9-94ac-3b2fb51eb989",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('We', 'are', 'learner', 'of', 'AI', 'from', '12th', 'June'), ('are', 'learner', 'of', 'AI', 'from', '12th', 'June', '2025'), ('learner', 'of', 'AI', 'from', '12th', 'June', '2025', 'till'), ('of', 'AI', 'from', '12th', 'June', '2025', 'till', 'now')]\n"
     ]
    }
   ],
   "source": [
    "# N-grams (n=8)\n",
    "quotes_tokens_n = list(nltk.ngrams(quotes_tokens, 8))\n",
    "print(quotes_tokens_n)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0258bf0-bb90-4834-b9b1-782aadbb9eb8",
   "metadata": {},
   "source": [
    "# Stemming\n",
    "**Definition:** Reduces words to their root form, often by chopping off suffixes.\n",
    "Types:\n",
    "\n",
    "- **Porter Stemmer** – Basic and widely used, but may not handle all words well.\n",
    "- **Lancaster Stemmer** – More aggressive, sometimes over-stems words.\n",
    "- **Snowball Stemmer** – Advanced, supports multiple languages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cebe0edf-7449-4219-8697-30b91fcc02e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['given',\n",
       " 'give',\n",
       " 'giving',\n",
       " 'gave',\n",
       " 'thinking',\n",
       " 'loving',\n",
       " 'maximum',\n",
       " 'shalu',\n",
       " 'gaved']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer, LancasterStemmer, SnowballStemmer\n",
    "\n",
    "words_to_stem = ['given','give','giving','gave','thinking','loving','maximum','shalu','gaved']\n",
    "words_to_stem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76e0a067-fed0-4b27-811b-2119e5081bbe",
   "metadata": {},
   "source": [
    "## Porter Stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c3ecf1bf-4086-4a2b-9f20-9d53d80e5b37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "given : given\n",
      "give : give\n",
      "giving : give\n",
      "gave : gave\n",
      "thinking : think\n",
      "loving : love\n",
      "maximum : maximum\n",
      "shalu : shalu\n",
      "gaved : gave\n"
     ]
    }
   ],
   "source": [
    "# Porter Stemmer\n",
    "pst = PorterStemmer()\n",
    "for word in words_to_stem:\n",
    "    print(word + ' : ' + pst.stem(word))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9b6445b-8aa5-45db-838e-e9d8df5bb460",
   "metadata": {},
   "source": [
    "## Lancaster Stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a2cf82e3-1af3-4e87-8961-b48ed752c6a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "given : giv\n",
      "give : giv\n",
      "giving : giv\n",
      "gave : gav\n",
      "thinking : think\n",
      "loving : lov\n",
      "maximum : maxim\n",
      "shalu : shalu\n",
      "gaved : gav\n"
     ]
    }
   ],
   "source": [
    "# Lancaster Stemmer\n",
    "lst = LancasterStemmer()\n",
    "for word in words_to_stem:\n",
    "    print(word + ' : ' + lst.stem(word))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4091e3ed-6e5d-4658-b6d9-9e91639f6740",
   "metadata": {},
   "source": [
    "## Snowball Stemmer (English)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1f63821e-f7f5-4ea6-ac08-9bc5c6c85ce2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "given : given\n",
      "give : give\n",
      "giving : give\n",
      "gave : gave\n",
      "thinking : think\n",
      "loving : love\n",
      "maximum : maximum\n",
      "shalu : shalu\n",
      "gaved : gave\n"
     ]
    }
   ],
   "source": [
    "# Snowball Stemmer (English)\n",
    "sbst = SnowballStemmer('english')\n",
    "for word in words_to_stem:\n",
    "    print(word + ' : ' + sbst.stem(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9b461eb5-2040-45da-ac48-4ffd3c84b098",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "samstag\n"
     ]
    }
   ],
   "source": [
    "# Snowball Stemmer (German)\n",
    "stemmer = SnowballStemmer('german')\n",
    "print(stemmer.stem('samstag'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e34a670c-5f9a-4701-ab1f-6864dbbb2373",
   "metadata": {},
   "source": [
    "# Lemmatization\n",
    "**Definition:** Reduces words to their dictionary (lemma) form, considering meaning and grammar.\n",
    "\n",
    "- More accurate than stemming because it uses vocabulary and morphological analysis.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ee00dc9d-c209-4db4-b845-b2f328fb3dc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "given : given\n",
      "give : give\n",
      "giving : giving\n",
      "gave : gave\n",
      "thinking : thinking\n",
      "loving : loving\n",
      "maximum : maximum\n",
      "shalu : shalu\n",
      "gaved : gaved\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "word_lem = WordNetLemmatizer()\n",
    "for word in words_to_stem:\n",
    "    print(word + ' : ' + word_lem.lemmatize(word))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e1288be-d6b6-471b-857c-0e7bb5daf846",
   "metadata": {},
   "source": [
    "# Stopwords\n",
    "**Definition:** Commonly used words (e.g., \"the\", \"is\", \"in\") that are often removed in NLP tasks.\n",
    "- Removing stopwords helps focus on meaningful content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e0acd97f-5703-40b2-bb7a-0f723d29bb8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'about', 'above', 'after', 'again', 'against', 'ain', 'all', 'am', 'an']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "print(stopwords.words('english')[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0ed2b33a-a035-40bd-82d4-4ed4c24f1736",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "198"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int(len(stopwords.words('english')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2354c4a1-0ae8-44e6-8432-304b131dee63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopwords length for French is 157\n",
      "Stopwords length for German is 232\n",
      "Stopwords length for Chinese is 841\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "print(\"Stopwords length for French is\", len(stopwords.words('french')))\n",
    "print(\"Stopwords length for German is\", len(stopwords.words('german')))\n",
    "print(\"Stopwords length for Chinese is\", len(stopwords.words('chinese')))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2aff533-e2dc-4087-91ee-f6e2a8065d8a",
   "metadata": {},
   "source": [
    "# POS Tagging\n",
    "**Definition:** Assigns grammatical labels (noun, verb, adjective, etc.) to each token in a sentence.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "87beb170-9285-4e06-831e-69e6be46c0b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('The', 'DT'), ('quick', 'JJ'), ('brown', 'NN'), ('fox', 'NN'), ('jumps', 'VBZ'), ('over', 'IN'), ('the', 'DT'), ('lazy', 'JJ'), ('dog', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "sentence = \"The quick brown fox jumps over the lazy dog\"\n",
    "tokens = word_tokenize(sentence)\n",
    "pos_tags = nltk.pos_tag(tokens)\n",
    "\n",
    "print(pos_tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abb09651-6fc3-40f2-a853-ed39c296a111",
   "metadata": {},
   "source": [
    "# Named Entity Recognition (NER)\n",
    "**Definition:** Identifies and classifies entities in text (people, places, organizations, dates, etc.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fa6ff9f2-557d-4754-b9f4-5e92e80d5f3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  The/DT\n",
      "  (GSP US/NNP)\n",
      "  president/NN\n",
      "  stays/NNS\n",
      "  in/IN\n",
      "  the/DT\n",
      "  (ORGANIZATION WHITEHOUSE/NNP))\n"
     ]
    }
   ],
   "source": [
    "from nltk import ne_chunk\n",
    "\n",
    "NE_sent = \"The US president stays in the WHITEHOUSE\"\n",
    "NE_tokens = word_tokenize(NE_sent)\n",
    "NE_tags = nltk.pos_tag(NE_tokens)\n",
    "NE_NER = ne_chunk(NE_tags)\n",
    "\n",
    "print(NE_NER)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94956d2a-46bb-44ae-bd30-27968e080a86",
   "metadata": {},
   "source": [
    "# Syntax in NLP\n",
    "**Definition:** The set of rules, principles, and processes that govern the structure of sentences in a language.\n",
    "\n",
    "**Example Rule:** In English, a simple sentence follows Subject → Verb → Object.\n",
    "\n",
    "**Example:** \"She (S) eats (V) apples (O).\"\n",
    "\n",
    "**In NLP:** Syntax parsing helps a machine understand how words relate to each other grammatically, which is important for translation, question answering, and information extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbe2ba74-f290-49f5-8a14-41f119914fc3",
   "metadata": {},
   "source": [
    "**Summary**\n",
    "  \n",
    "* Learned WhitespaceTokenizer and wordpunct_tokenize for flexible token splitting.\n",
    "* Explored n-grams for sequence-based analysis.\n",
    "* Compared Porter, Lancaster, and Snowball stemmers.\n",
    "* Understood lemmatization for accurate root forms.\n",
    "* Reviewed stopwords in multiple languages.\n",
    "* Applied POS tagging to identify grammatical roles of words.\n",
    "* Performed Named Entity Recognition (NER) to detect people, places, and organizations.\n",
    "* Discussed syntax as the set of rules and principles that define sentence structure in NLP."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
